# -*- coding: utf-8 -*-
"""Legal_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/groot2193/LawSage_AI/blob/main/Legal_rag.ipynb

# Installing required modules
"""

!pip install groq
!pip install gradio
!pip install langchain-groq
!pip install langchain
!pip install langchain-community
!pip install faiss-gpu
!pip install faiss-cpu
!pip install pypdf
!pip install sentence_transformers

"""# Importing Modules"""

import os
import gradio as gr
from langchain_groq import ChatGroq
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
import numpy as np

"""# Connector"""

groq_api = "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"


llm = ChatGroq(
    api_key=groq_api,
    model="Llama3-8b-8192"
)

prompt = ChatPromptTemplate.from_template(
"""
Answer the questions based on the provided context only.
Please provide the most accurate response based on the question
<context>
{context}
<context>
Questions:{input}

"""
)

"""# States"""

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load the PDF file
loader = PyPDFLoader("/basic-laws-book-2016.pdf")
docs = loader.load()

# Check how many documents were loaded and display the first one
print(f"Loaded {len(docs)} documents.")
if docs:
    print("Sample document text:", docs[0].page_content[:500])  # Displaying first 500 characters of first document

# Define the text splitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# Split the documents into smaller chunks
final_docs = text_splitter.split_documents(docs)


# Print the first chunk to verify the split
print(f"Number of chunks: {len(final_docs)}")
if final_docs:
    print("Sample chunk text:", final_docs[0].page_content[:500])  # Display first chunk text

embeddings = HuggingFaceEmbeddings()
vectors = FAISS.from_documents(final_docs,embeddings)

np.array(vectors)

def getAns(prompt1, chat_history=None):
    # Combine previous chat history with the new question if available
    if chat_history is None:
        chat_history = []

    document_chain = create_stuff_documents_chain(llm, prompt)
    retriever = vectors.as_retriever()
    retrival_chain = create_retrieval_chain(retriever, document_chain)

    # Get the response
    response = retrival_chain.invoke({"input": prompt1})
    answer = response["answer"]

    # Append the question and answer to chat history for continuity
    chat_history.append(("You:", prompt1))
    chat_history.append(("Bot:", answer))

    return answer, chat_history

getAns("what is the first president act?")

def gradio_interface(query, chat_history=None):
    answer, chat_history = getAns(query, chat_history)

    # Display the question and answer on the interface
    conversation = ""
    for q, a in chat_history:
        conversation += f"{q} {a}\n"

    return conversation, chat_history

# Setup Gradio Interface with state to track conversation
interface = gr.Interface(
    fn=gradio_interface,
    inputs=[gr.Textbox(label="Ask a Legal Question", placeholder="Type your question here..."),
            gr.State()],  # State to store chat history
    outputs=[gr.Textbox(label="Conversation History", interactive=False),
             gr.State()],  # State to store chat history and pass it along
    title="Legal Document Helper Chatbot",
    description="Ask questions related to legal documents, and the chatbot will respond based on the context of the documents you provide.",
    allow_flagging="never"
)

# Launch Gradio interface
interface.launch()

